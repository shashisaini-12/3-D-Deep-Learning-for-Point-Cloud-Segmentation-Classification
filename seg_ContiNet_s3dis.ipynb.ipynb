{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ContiNet for Segmentation**\n",
    "\n",
    "This notebook will explore the Segmentation head of Point Net using the reduced and paritioned version of S3DIS dataset. This version is similar to the one used by torch geometry except that we will have to normalize and sample data on the fly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "import open3d as o3\n",
    "# from open3d import JVisualizer # For Colab Visualization\n",
    "from open3d.web_visualizer import draw # for non Colab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ROOT = r\"write/the/path/for/s3dis/dataset/folder\"\n",
    "\n",
    "# feature selection hyperparameters\n",
    "NUM_TRAIN_POINTS = 4096 # train/valid points\n",
    "# NUM_TEST_POINTS = 15000\n",
    "NUM_TEST_POINTS = 15000  # 15000\n",
    "\n",
    "BATCH_SIZE = 16 # Original is 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'ceiling'  : 0, \n",
    "    'floor'    : 1, \n",
    "    'wall'     : 2, \n",
    "    'beam'     : 3, \n",
    "    'column'   : 4, \n",
    "    'window'   : 5,\n",
    "    'door'     : 6, \n",
    "    'table'    : 7, \n",
    "    'chair'    : 8, \n",
    "    'sofa'     : 9, \n",
    "    'bookcase' : 10, \n",
    "    'board'    : 11,\n",
    "    'stairs'   : 12,\n",
    "    'clutter'  : 13\n",
    "}\n",
    "\n",
    "# unique color map generated via\n",
    "# https://mokole.com/palette.html\n",
    "COLOR_MAP = {\n",
    "    0  : (47, 79, 79),    # ceiling - darkslategray\n",
    "    1  : (139, 69, 19),   # floor - saddlebrown\n",
    "    2  : (34, 139, 34),   # wall - forestgreen\n",
    "    3  : (75, 0, 130),    # beam - indigo\n",
    "    4  : (255, 0, 0),     # column - red \n",
    "    5  : (255, 255, 0),   # window - yellow\n",
    "    6  : (0, 255, 0),     # door - lime\n",
    "    7  : (0, 255, 255),   # table - aqua\n",
    "    8  : (0, 0, 255),     # chair - blue\n",
    "    9  : (255, 0, 255),   # sofa - fuchsia\n",
    "    10 : (238, 232, 170), # bookcase - palegoldenrod\n",
    "    11 : (100, 149, 237), # board - cornflower\n",
    "    12 : (255, 105, 180), # stairs - hotpink\n",
    "    13 : (0, 0, 0)        # clutter - black\n",
    "}\n",
    "\n",
    "v_map_colors = np.vectorize(lambda x : COLOR_MAP[x])\n",
    "\n",
    "NUM_CLASSES = len(CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from s3dis_dataset import S3DIS\n",
    "\n",
    "# get datasets\n",
    "s3dis_train = S3DIS(ROOT, area_nums='1-4', npoints=NUM_TRAIN_POINTS, split='train', r_prob=0.25)\n",
    "s3dis_valid = S3DIS(ROOT, area_nums='5', npoints=NUM_TRAIN_POINTS, split='valid', r_prob=0.)\n",
    "s3dis_test = S3DIS(ROOT, area_nums='6', split='test', npoints=NUM_TEST_POINTS)\n",
    "\n",
    "# get dataloaders\n",
    "train_dataloader = DataLoader(s3dis_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(s3dis_valid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(s3dis_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, targets = s3dis_train[6]  # Here 10 is random number \n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets)).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualizaton\n",
    "#### 1. Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_targets = []\n",
    "for (_, targets) in train_dataloader:\n",
    "    total_train_targets += targets.reshape(-1).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_bins = np.bincount(total_train_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), train_class_bins,\n",
    "        color=[np.array(val)/255. for val in list(COLOR_MAP.values())],\n",
    "        edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Frequency of Each Category (Training - Areas 1-4)', size=14, pad=20)\n",
    "\n",
    "train_data_dict = {}\n",
    "for i in CATEGORIES:\n",
    "    train_data_dict[i] = train_class_bins[CATEGORIES[i]]\n",
    "print('Train Class Count:- ', train_data_dict, sep='\\n')\n",
    "print('Total train instances :', np.sum(train_class_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Valid Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valid_targets = []\n",
    "for (_, targets) in valid_dataloader:\n",
    "    total_valid_targets += targets.reshape(-1).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_class_bins = np.bincount(total_valid_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), valid_class_bins,\n",
    "        color=[np.array(val)/255. for val in list(COLOR_MAP.values())],\n",
    "        edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Frequency of Each Category (Validation - Areas 6)', size=14, pad=20)\n",
    "\n",
    "valid_data_dict = {}\n",
    "for i in CATEGORIES:\n",
    "    valid_data_dict[i] = valid_class_bins[CATEGORIES[i]]\n",
    "print('Train Class Count:- ', valid_data_dict, sep='\\n')\n",
    "print('Total train instances :', np.sum(valid_class_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_total_test_targets = []\n",
    "for (_, targets) in test_dataloader:\n",
    "    _total_test_targets += targets.reshape(-1).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_bins = np.bincount(_total_test_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), test_class_bins,\n",
    "        color=[np.array(val)/255. for val in list(COLOR_MAP.values())],\n",
    "        edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Frequency of Each Category (Testing - Areas 5)', size=14, pad=20)\n",
    "\n",
    "test_data_dict = {}\n",
    "for i in CATEGORIES:\n",
    "    test_data_dict[i] = test_class_bins[CATEGORIES[i]]\n",
    "print('Train Class Count:- ', test_data_dict, sep='\\n')\n",
    "print('Total train instances :', np.sum(test_class_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Segmentation Version of ContiNet\n",
    "Make a test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from continet import ContiNetSegmentation\n",
    "\n",
    "points, targets = next(iter(train_dataloader))\n",
    "print(f\"Shape of points: {points.shape}  & \\n Shape of targets: {targets.shape}\")\n",
    "seg_model = ContiNetSegmentation(num_points=NUM_TRAIN_POINTS, m=NUM_CLASSES)\n",
    "out, _, _ = seg_model(points.transpose(2, 1))\n",
    "print(f'Seg shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from point_net_loss import PointNetSegLoss\n",
    "\n",
    "#EPOCHS = 100\n",
    "\n",
    "EPOCHS = 100\n",
    "LR = 0.00005  # 0.0001\n",
    "\n",
    "# use inverse class weighting\n",
    "alpha = 1 / train_class_bins\n",
    "alpha = (alpha/alpha.max())\n",
    "\n",
    "# manually set alpha weights\n",
    "#alpha = np.ones(len(CATEGORIES))\n",
    "#alpha[2] *= 0.25 # balance background class (Wall)\n",
    "#alpha[8] *= 0.25 # balance background class (Chair)\n",
    "#alpha[-1] *= 0.75  # balance clutter class (Clutter)\n",
    "\n",
    "gamma = 1.01\n",
    "\n",
    "optimizer = optim.Adam(seg_model.parameters(), lr=LR)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, \n",
    " #                                             step_size_up=1000, cycle_momentum=False)\n",
    "criterion = PointNetSegLoss(alpha=alpha, gamma=gamma, dice=True, size_average=False).to(DEVICE)\n",
    "\n",
    "seg_model = seg_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOU calculation\n",
    "\n",
    "def compute_iou(targets, predictions):\n",
    "\n",
    "    targets = targets.reshape(-1)\n",
    "    predictions = predictions.reshape(-1)\n",
    "\n",
    "    intersection = torch.sum(predictions == targets) # true positives\n",
    "    union = len(predictions) + len(targets) - intersection\n",
    "\n",
    "    return intersection / union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store best validation iou\n",
    "#best_iou = 0.6\n",
    "#best_mcc = 0.6\n",
    "\n",
    "best_iou = 0.0\n",
    "best_mcc = 0.0\n",
    "best_acc = 0.0\n",
    "\n",
    "# lists to store metrics\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "train_mcc = []\n",
    "train_iou = []\n",
    "valid_loss = []\n",
    "valid_accuracy = []\n",
    "valid_mcc = []\n",
    "valid_iou = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff for training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_train_batch = int(np.ceil(len(s3dis_train)/BATCH_SIZE))\n",
    "num_valid_batch = int(np.ceil(len(s3dis_valid)/BATCH_SIZE))\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS+1)):                # for epoch in range(1, EPOCHS):\n",
    "    # place model in training mode\n",
    "    seg_model = seg_model.train()\n",
    "    _train_loss = []\n",
    "    _train_accuracy = []\n",
    "    _train_mcc = []\n",
    "    _train_iou = []\n",
    "\n",
    "    for i, (points, targets) in enumerate(train_dataloader, 0):\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get predicted class logits\n",
    "        preds, _, _ = seg_model(points)\n",
    "\n",
    "        # get class prediction\n",
    "        pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "        # get loss and perform backprop\n",
    "        loss = criterion(preds, targets, pred_choice)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        # get metrics\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
    "        mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "        iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "        # update epoch loss and accuracy\n",
    "        _train_loss.append(loss.item())\n",
    "        _train_accuracy.append(accuracy)\n",
    "        _train_mcc.append(mcc.item())\n",
    "        _train_iou.append(iou.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'\\t [{epoch}: {i}/{num_train_batch}] '\\\n",
    "                  + f'train loss: {loss.item():.4f} '\\\n",
    "                  + f'accuracy: {accuracy:.4f} '\\\n",
    "                  + f'mccc: {mcc:.4f} '\\\n",
    "                  + f'iou: {iou:.4f}')\n",
    "            \n",
    "    train_loss.append(np.mean(_train_loss))\n",
    "    train_accuracy.append(np.mean(_train_accuracy))\n",
    "    train_mcc.append(np.mean(_train_mcc))\n",
    "    train_iou.append(np.mean(_train_iou))\n",
    "\n",
    "    print(f'Epoch: {epoch} - Train Loss: {train_loss[-1]:.4f} ' \\\n",
    "          + f'- Train Accuracy: {train_accuracy[-1]:.4f} ' \\\n",
    "          + f'- Train MCC: {train_mcc[-1]:.4f} ' \\\n",
    "          + f'- Train IOU: {train_iou[-1]:.4f}')\n",
    "    \n",
    "    # pause to cool down\n",
    "    time.sleep(3)\n",
    "\n",
    "    # get test results after each epoch\n",
    "    with torch.no_grad():\n",
    "        # place model in evaluation mode\n",
    "        seg_model = seg_model.eval()\n",
    "        _valid_loss = []\n",
    "        _valid_accuracy = []\n",
    "        _valid_mcc = []\n",
    "        _valid_iou = []\n",
    "\n",
    "        for i, (points, targets) in enumerate(valid_dataloader, 0):\n",
    "            points = points.transpose(2, 1).to(DEVICE)\n",
    "            targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "            preds, _, A = seg_model(points)\n",
    "            pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "            loss = criterion(preds, targets, pred_choice)\n",
    "\n",
    "            # get metrics\n",
    "            correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "            accuracy = correct/float(BATCH_SIZE*NUM_TRAIN_POINTS)\n",
    "            mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "            iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "            # update epoch loss and accuracy\n",
    "            _valid_loss.append(loss.item())\n",
    "            _valid_accuracy.append(accuracy)\n",
    "            _valid_mcc.append(mcc.item())\n",
    "            _valid_iou.append(iou.item())\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'\\t [{epoch}: {i}/{num_valid_batch}] ' \\\n",
    "                  + f'valid loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} '\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')\n",
    "\n",
    "        valid_loss.append(np.mean(_valid_loss))\n",
    "        valid_accuracy.append(np.mean(_valid_accuracy))\n",
    "        valid_mcc.append(np.mean(_valid_mcc))\n",
    "        valid_iou.append(np.mean(_valid_iou))\n",
    "        print(f'Epoch: {epoch} - Valid Loss: {valid_loss[-1]:.4f} ' \\\n",
    "              + f'- Valid Accuracy: {valid_accuracy[-1]:.4f} ' \\\n",
    "              + f'- Valid MCC: {valid_mcc[-1]:.4f} ' \\\n",
    "              + f'- Valid IOU: {valid_iou[-1]:.4f}')\n",
    "    # pause to cool down\n",
    "\n",
    "    # get the current validation accuracy, mcc & iou\n",
    "    current_acc = valid_accuracy[-1]\n",
    "    current_mcc = valid_mcc[-1]\n",
    "    current_iou = valid_iou[-1] \n",
    "\n",
    "    # Check if the current accuracy is better than the best so far\n",
    "    if current_acc > best_acc:\n",
    "        best_acc = current_acc\n",
    "        best_acc_model_state = seg_model.state_dict()\n",
    "    \n",
    "    # Check if the current mcc is better than the best so far\n",
    "    if current_mcc > best_mcc:\n",
    "        best_mcc = current_mcc\n",
    "        best_mcc_model_state = seg_model.state_dict()\n",
    "\n",
    "    # Check if the current iou is better than the best so far\n",
    "    if current_iou > best_iou:\n",
    "        best_iou = current_iou\n",
    "        best_iou_model_state = seg_model.state_dict()\n",
    "\n",
    "# saving the best three model based on based accuracy, mcc & iou\n",
    "model_name_1 = 'continet_best_seg_acc_model_01.pth'\n",
    "model_name_2 = 'continet_best_seg_mcc_model_01.pth'\n",
    "model_name_3 = 'continet_best_seg_iou_model_01.pth'\n",
    "\n",
    "path_1 = os.path.join(os.getcwd(), model_name_1)\n",
    "path_2 = os.path.join(os.getcwd(), model_name_2)\n",
    "path_3 = os.path.join(os.getcwd(), model_name_3)\n",
    "\n",
    "torch.save(best_acc_model_state, path_1)\n",
    "torch.save(best_mcc_model_state, path_2)\n",
    "torch.save(best_iou_model_state, path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(8, 5))\n",
    "ax[0].plot(np.arange(1, EPOCHS + 1), train_loss, label='train')\n",
    "ax[0].plot(np.arange(1, EPOCHS + 1), valid_loss, label='valid')\n",
    "ax[0].set_title('loss')\n",
    "\n",
    "ax[1].plot(np.arange(1, EPOCHS + 1), train_accuracy)\n",
    "ax[1].plot(np.arange(1, EPOCHS + 1), valid_accuracy)\n",
    "ax[1].set_title('accuracy')\n",
    "\n",
    "ax[2].plot(np.arange(1, EPOCHS + 1), train_mcc)\n",
    "ax[2].plot(np.arange(1, EPOCHS + 1), valid_mcc)\n",
    "ax[2].set_title('mcc')\n",
    "\n",
    "ax[3].plot(np.arange(1, EPOCHS + 1), train_iou)\n",
    "ax[3].plot(np.arange(1, EPOCHS + 1), valid_iou)\n",
    "ax[3].set_title('iou')\n",
    "\n",
    "fig.legend(loc='upper right')\n",
    "plt.subplots_adjust(wspace=0., hspace=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name_1 = \"continet_best_seg_acc_model_01.pth\"\n",
    "MODEL_PATH_acc = os.path.join(os.getcwd(), model_name_1)\n",
    "#MODEL_PATH_mcc = os.path.join(os.getcwd(), model_name_2)\n",
    "#MODEL_PATH_iou = os.path.join(os.getcwd(), model_name_3)\n",
    "\n",
    "\n",
    "model = ContiNetSegmentation(num_points=NUM_TEST_POINTS, m=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH_acc))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Quick test to gather metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_batch = int(np.ceil(len(s3dis_test)/BATCH_SIZE))\n",
    "\n",
    "total_test_targets = []\n",
    "total_test_preds = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # place model in evaluation mode\n",
    "    model = model.eval()\n",
    "\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    test_mcc = []\n",
    "    test_iou = []\n",
    "    for i, (points, targets) in enumerate(test_dataloader, 0):\n",
    "\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "        preds, _, A = model(points)\n",
    "        pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "        loss = criterion(preds, targets, pred_choice)\n",
    "\n",
    "        # get metrics\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct/float(BATCH_SIZE*NUM_TEST_POINTS)\n",
    "        mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "        iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "        # update epoch loss and accuracy\n",
    "        test_loss.append(loss.item())\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_mcc.append(mcc.item())\n",
    "        test_iou.append(iou.item())\n",
    "\n",
    "        # add to total targets/preds\n",
    "        total_test_targets += targets.reshape(-1).cpu().numpy().tolist()\n",
    "        total_test_preds += pred_choice.reshape(-1).cpu().numpy().tolist()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'\\t [{i}/{num_test_batch}] ' \\\n",
    "                  + f'test loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} ' \\\n",
    "                  + f'mcc: {mcc:.4f} ' \\\n",
    "                  + f'iou: {iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test results\n",
    "print(f'Test Loss: {np.mean(test_loss):.4f} ' \\\n",
    "        + f'- Test Accuracy: {np.mean(test_accuracy):.4f} ' \\\n",
    "        + f'- Test MCC: {np.mean(test_mcc):.4f} ' \\\n",
    "        + f'- Test IOU: {np.mean(test_iou):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_targets = np.array(total_test_targets)\n",
    "total_test_preds = np.array(total_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Confusion Matrix for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_confusion = pd.DataFrame(confusion_matrix(total_test_targets, total_test_preds),\n",
    "                              columns=list(CATEGORIES.keys()),\n",
    "                              index=list(CATEGORIES.keys()))\n",
    "\n",
    "test_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat MaP Analysis\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(test_confusion, annot=True, cmap='Greens')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per class Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View test results on full space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # release GPU memory\n",
    "points, targets = s3dis_test.get_random_partitioned_space()\n",
    "\n",
    "# place on device\n",
    "points = points.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "# Normalize each partitioned Point Cloud to (0, 1)\n",
    "norm_points = points.clone()\n",
    "norm_points = norm_points - norm_points.min(axis=1)[0].unsqueeze(1)\n",
    "norm_points /= norm_points.max(axis=1)[0].unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # prepare data\n",
    "    norm_points = norm_points.transpose(2, 1)\n",
    "    targets = targets.squeeze()\n",
    "\n",
    "    # run inference\n",
    "    preds, _, _ = model(norm_points)\n",
    "\n",
    "    # get metrics\n",
    "    pred_choice = torch.softmax(preds, dim=2).argmax(dim=2)\n",
    "\n",
    "    loss = criterion(preds, targets, pred_choice)\n",
    "    correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "    accuracy = correct/float(points.shape[0]*NUM_TEST_POINTS)\n",
    "    mcc = mcc_metric(preds.transpose(2, 1), targets)\n",
    "    iou = compute_iou(targets, pred_choice)\n",
    "\n",
    "print(f'Loss: {loss:.4f} - Accuracy: {accuracy:.4f} - MCC: {mcc:.4f} - IOU: {iou:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true full point cloud\n",
    "\"\"\"pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw(pcd)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'points' and 'targets' are already defined\n",
    "# You may need to adjust the following lines based on your actual data\n",
    "\n",
    "# Create a Python list from the torch.Tensor\n",
    "points_list = points.permute(2, 0, 1).reshape(3, -1).to('cpu').T.tolist()\n",
    "\n",
    "# Create a Python list from the numpy array\n",
    "colors_list = np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T / 255\n",
    "\n",
    "# Create o3.utility.Vector3dVector objects\n",
    "pcd_points = o3.utility.Vector3dVector(points_list)\n",
    "pcd_colors = o3.utility.Vector3dVector(colors_list)\n",
    "\n",
    "# Create the PointCloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = pcd_points\n",
    "pcd.colors = pcd_colors\n",
    "\n",
    "# Now you can visualize the point cloud\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Save point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.io.write_point_cloud('full.pcd', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true partitioned point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.to('cpu')[2, :, :])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.to('cpu')[2, :])).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted full point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.io.write_point_cloud('full_predicted.pcd', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted partitioned point cloud\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points.to('cpu')[2, :, :])\n",
    "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.to('cpu')[2, :])).T/255)\n",
    "\n",
    "# draw(pcd)\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try displaying both point clouds??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display true full point cloud\n",
    "pcd_1 = o3.geometry.PointCloud()\n",
    "pcd_1.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T)\n",
    "pcd_1.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(targets.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "# display predicted full point cloud\n",
    "pcd_2 = o3.geometry.PointCloud()\n",
    "pcd_2.points = o3.utility.Vector3dVector(points.permute(2, 0, 1).reshape(3, -1).to('cpu').T + torch.Tensor([5, 0, 0]))\n",
    "pcd_2.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(pred_choice.reshape(-1).to('cpu'))).T/255)\n",
    "\n",
    "draw([pcd_1] + [pcd_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the critical indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too resource extensive----------------------------------be careful\n",
    "\"\"\"\n",
    "BaseExceptionGrouptorch.cuda.empty_cache() # release GPU memory\n",
    "points, targets = s3dis_test.get_random_partitioned_space()\n",
    "\n",
    "place on device\n",
    "points = points.to(DEVICE)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "Normalize each partitioned Point Cloud to (0, 1)\n",
    "norm_points = points.clone()\n",
    "norm_points = norm_points - norm_points.min(axis=1)[0].unsqueeze(1)\n",
    "norm_points /= norm_points.max(axis=1)[0].unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    prepare data\n",
    "    norm_points = norm_points.transpose(2, 1)\n",
    "    targets = targets.squeeze()\n",
    "\n",
    "    run inference to get critical indexes\n",
    "    _, crit_idxs, _ = model(norm_points)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate data into single point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = []\n",
    "#crit_pcds = []\n",
    "for i in range(points.shape[0]):\n",
    "    \n",
    "    pts = points[i, :]\n",
    "    #cdx = crit_idxs[i, :]\n",
    "    tgt = targets[i, :]\n",
    "\n",
    "    # get full point clouds\n",
    "    pcd = o3.geometry.PointCloud()\n",
    "    pcd.points = o3.utility.Vector3dVector(pts)\n",
    "    pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(tgt)).T/255)\n",
    "\n",
    "    # get critical set point clouds\n",
    "    #critical_points = pts[cdx, :]\n",
    "    #critical_point_colors = np.vstack(v_map_colors(tgt[cdx])).T/255\n",
    "\n",
    "   # crit_pcd = o3.geometry.PointCloud()\n",
    "   # crit_pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "    #crit_pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "    pcds.append(pcd)\n",
    "    #crit_pcds.append(crit_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(pcds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "pcds_combined = pcds[0]\n",
    "for p in pcds[1:]:\n",
    "    pcds_combined += p\n",
    "\n",
    "o3.io.write_point_cloud('full_set.pcd', pcds_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw critical indexes of a single partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = points[0, :].to('cpu')\n",
    "cdx = crit_idxs[0, :].to('cpu')\n",
    "tgt = targets[0, :].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_points = pts[cdx, :]\n",
    "critical_point_colors = np.vstack(v_map_colors(tgt[cdx])).T/255\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "# o3.visualization.draw_plotly([pcd])\n",
    "# draw(pcd, point_size=5) # does not work in Colab\n",
    "draw(pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw Critical Indexes of the entire Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(crit_pcds, point_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "pcds_combined = pcds[0]\n",
    "for p in pcds[1:]:\n",
    "    pcds_combined += p\n",
    "\n",
    "o3.io.write_point_cloud('critical_set.pcd', pcds_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
