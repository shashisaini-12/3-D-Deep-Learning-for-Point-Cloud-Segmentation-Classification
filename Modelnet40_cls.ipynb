{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContiNet -ModelNets_40_Classificiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "import open3d as o3\n",
    "\n",
    "from open3d.web_visualizer import draw # for non Colab\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to initialize all the random sequence from always same point\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# General parameters\n",
    "NUM_TRAIN_POINTS = 2500  # 4096\n",
    "NUM_TEST_POINTS = 10000\n",
    "NUM_CLASSES = 40\n",
    "ROOT = r\"\"\n",
    "ROOT = r'C:\\Users\\DIAT-YSD-DS\\Desktop\\PointNet_Shashi\\PointNet_Dataset\\ModelNet40_point_cloud_data'\n",
    "\n",
    "GLOBAL_FEATS = 1600\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class label mapping\n",
    "CATEGORIES = {'airplane': 0, 'bathtub': 1, 'bed': 2, 'bench': 3, 'bookshelf': 4, 'bottle': 5, 'bowl': 6,\n",
    "              'car': 7, 'chair': 8, 'cone': 9, 'cup': 10, 'curtain': 11, 'desk': 12, 'door': 13,\n",
    "              'dresser': 14, 'flower_pot': 15, 'glass_box': 16, 'guitar': 17, 'keyboard': 18,\n",
    "              'lamp': 19, 'laptop': 20, 'mantel': 21, 'monitor': 22, 'night_stand': 23, 'person': 24, \n",
    "              'piano': 25, 'plant': 26, 'radio': 27, 'range_hood': 28, 'sink': 29, 'sofa': 30,\n",
    "              'stairs': 31, 'stool': 32, 'table': 33, 'tent': 34, 'toilet': 35, 'tv_stand': 36,\n",
    "              'vase': 37, 'wardrobe': 38, 'xbox': 39}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from Augmentation_modelnet40_dataloader import Modelnet40Dataset\n",
    "\n",
    "# train Dataset & Data Loader\n",
    "train_dataset = Modelnet40Dataset(ROOT, npoints=NUM_TRAIN_POINTS, split='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# valid Dataset & Data loader\n",
    "valid_dataset = Modelnet40Dataset(ROOT, npoints=NUM_TRAIN_POINTS, split='valid')\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# test Dataset & Data Loader\n",
    "test_dataset = Modelnet40Dataset(ROOT, npoints=NUM_TEST_POINTS, split='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = Modelnet40Dataset(ROOT, npoints=5000, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, target = sample_dataset[-500]\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points)\n",
    "print(\"Label: \", next(k for k,v in CATEGORIES.items() if v == target))\n",
    "o3.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization,\n",
    "#### 1. Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_targets = []\n",
    "for (_, targets) in train_dataloader: \n",
    "    total_train_targets += targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_bins = np.bincount(total_train_targets)\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab20', NUM_CLASSES)\n",
    "colors = [cmap(i) for i in range(NUM_CLASSES-1)]\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(list(CATEGORIES.keys()), train_class_bins,\n",
    "        color=colors, width=0.7)\n",
    "\n",
    "#plt.bar(np.arrange(NUM_CLASSES), train_class_bins, width=0.5, color=colors, edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=13, rotation=90)\n",
    "plt.ylabel('Counts', size=13)\n",
    "plt.title('Train Class Frequencies', size=16, pad=20)\n",
    "\n",
    "train_data_dict = {}\n",
    "for i in CATEGORIES:\n",
    "    train_data_dict[i] = train_class_bins[CATEGORIES[i]]\n",
    "print(\"Train Class Count:-\", train_data_dict, sep='\\n')\n",
    "print(\"Total train instances :\", np.sum(train_class_bins))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. #### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valid_targets = []\n",
    "for (_, targets) in valid_dataloader: \n",
    "    total_valid_targets += targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_class_bins = np.bincount(total_valid_targets)\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab20', NUM_CLASSES)\n",
    "colors = [cmap(i) for i in range(NUM_CLASSES)]\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(list(CATEGORIES.keys()), valid_class_bins,\n",
    "        color=colors, width=0.7)\n",
    "\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=13, rotation=90)\n",
    "plt.ylabel('Counts', size=13)\n",
    "plt.title('Valid Class Frequencies', size=16, pad=20)\n",
    "\n",
    "valid_data_dict = {}\n",
    "for i in CATEGORIES:\n",
    "    valid_data_dict[i] = valid_class_bins[CATEGORIES[i]]\n",
    "print(\"Valid Class Count:-\", valid_data_dict, sep='\\n')\n",
    "print(\"Total valid instances :\", np.sum(valid_class_bins))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_total_test_targets = []\n",
    "for (_, targets) in test_dataloader:\n",
    "    _total_test_targets += targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_bins = np.bincount(_total_test_targets)\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab20', NUM_CLASSES)\n",
    "colors = [cmap(i) for i in range(NUM_CLASSES)]\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), test_class_bins, \n",
    "             color=colors, width=0.7)\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=13, rotation=90)\n",
    "plt.ylabel('Counts', size=13)\n",
    "plt.title('Test Class Frequencies', size=16, pad=20)\n",
    "\n",
    "test_data_dict = {}\n",
    "for i in CATEGORIES:\n",
    "    test_data_dict[i] = test_class_bins[CATEGORIES[i]]\n",
    "print(\"Test Class Count:-\", test_data_dict, sep='\\n')\n",
    "print(\"Total test instances :\", np.sum(test_class_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traing Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from continet import ContiNetClassification\n",
    "\n",
    "points, targets = next(iter(train_dataloader))\n",
    "classifier = ContiNetClassification(k=NUM_CLASSES, num_global_feats = GLOBAL_FEATS, num_points=NUM_TRAIN_POINTS)\n",
    "out, _, _ = classifier(points.transpose(2, 1))\n",
    "print(f'Class out shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from point_net_loss import PointNetLoss\n",
    "\n",
    "EPOCHS = 150\n",
    "LR = 0.00004\n",
    "REG_WEIGHT = 0.001  #0.0005\n",
    "\n",
    "# Use inverse class weighting\n",
    "alpha = 1/train_class_bins\n",
    "alpha = (alpha/alpha.max())\n",
    "\n",
    "gamma = 1 \n",
    "optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
    "\n",
    "#  This scheduler for GPU training only, else it would be very slow.\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, \n",
    "#                                              step_size_up=3800, cycle_momentum=False)\n",
    "\n",
    "criterion = PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=REG_WEIGHT, size_average=False).to(DEVICE)\n",
    "classifier = classifier.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Training Model\n",
    "First define a helper function to train, validate, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(classifier, dataloader, num_batch, epoch, split='train'):\n",
    "    \"\"\"Function to train or test the model\"\"\"\n",
    "\n",
    "    _loss = []\n",
    "    _accuracy = []\n",
    "    _mcc = []\n",
    "\n",
    "    ## return total targets and predictions for test case only\n",
    "    total_test_targets = []\n",
    "    total_test_preds = []\n",
    "\n",
    "    for i, (points, targets) in enumerate(dataloader, 0):\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get predicated class logits\n",
    "        preds, _, A = classifier(points)\n",
    "\n",
    "        # get loss and perform backprop\n",
    "        loss = criterion(preds, targets, A)\n",
    "\n",
    "        if split == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "        # get class prediction\n",
    "        pred_choice = torch.softmax(preds, dim=1).argmax(dim=1)\n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(BATCH_SIZE)\n",
    "        mcc = mcc_metric(preds, targets)\n",
    "\n",
    "        # Update epoch loss and accuracy\n",
    "        _loss.append(loss.item())\n",
    "        _accuracy.append(accuracy)\n",
    "        _mcc.append(mcc.item())\n",
    "\n",
    "        # add to total targets/preds\n",
    "        if split == 'test':\n",
    "            total_test_targets += targets.reshape(-1).cpu().tolist()\n",
    "            total_test_preds += pred_choice.reshape(-1).cpu().tolist()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'\\t [{epoch}: {i}/{num_batch}] '\\\n",
    "                  + f'{split} loss: {loss.item():.4f} '\\\n",
    "                    f'accuracy: {accuracy:.4f} mcc: {mcc:.4f}')\n",
    "    epoch_loss = np.mean(_loss)\n",
    "    epoch_accuracy = np.mean(_accuracy)\n",
    "    epoch_mcc = np.mean(_mcc)\n",
    "\n",
    "    print(f'Epoch: {epoch} - {split} Loss: {epoch_loss:.4f} '\\\n",
    "          + f' - {split} Accuracy: {epoch_accuracy:.4f} '\\\n",
    "            + f' - {split} MCC: {epoch_mcc:.4f}')\n",
    "    \n",
    "    if split == 'test':\n",
    "        return epoch_loss, epoch_accuracy, epoch_mcc, total_test_targets, total_test_preds\n",
    "    else:\n",
    "        return epoch_loss, epoch_accuracy, epoch_mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff for training\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_train_batch = int(np.ceil(len(train_dataset)/BATCH_SIZE))\n",
    "num_valid_batch = int(np.ceil(len(valid_dataset)/BATCH_SIZE))\n",
    "\n",
    "# lists to store metrics (loss, accuracy, mcc)\n",
    "train_metrics = []\n",
    "valid_metrics = []\n",
    "\n",
    "# Initialize variables to store the best validation accuracy, MCC, and their corresponding model states\n",
    "best_accuracy = 0\n",
    "best_valid_mcc = 0\n",
    "best_valid_accuracy = 0\n",
    "\n",
    "# Train on EPOCHS\n",
    "for epoch in tqdm(range(1, EPOCHS+1)):\n",
    "    ## train loop\n",
    "    _train_mertics = train_test(classifier, train_dataloader, num_train_batch, epoch, split='train')\n",
    "    train_metrics.append(_train_mertics)\n",
    "\n",
    "    ## pause to cool down\n",
    "    time.sleep(4)\n",
    "\n",
    "    ## validation loop\n",
    "    with torch.no_grad():\n",
    "        # place model in evaluation mode\n",
    "        classifier = classifier.eval()\n",
    "        # validate\n",
    "        _valid_metrics = train_test(classifier, valid_dataloader, num_valid_batch, epoch, split='valid')\n",
    "        valid_metrics.append(_valid_metrics)\n",
    "\n",
    "        # Get the current validation accuracy and MCC\n",
    "        current_valid_accuracy = _valid_metrics[1]\n",
    "        current_valid_mcc = _valid_metrics[-1]\n",
    "\n",
    "        # Check if the current validation accuracy is better than the best so far\n",
    "        if current_valid_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = current_valid_accuracy\n",
    "            best_acc_model_state = classifier.state_dict()\n",
    "\n",
    "        # Check if the current validation MCC is better than the best so far\n",
    "        if current_valid_mcc > best_valid_mcc:\n",
    "            best_valid_mcc = current_valid_mcc\n",
    "            best_mcc_model_state = classifier.state_dict()\n",
    "\n",
    "# pause to cool down\n",
    "time.sleep(4)\n",
    "\n",
    "# Save model with highest valid accuracy\n",
    "path = os.getcwd()\n",
    "filename_1 = \"continet_acc_cls_model_01.pth\"\n",
    "full_path_1 = os.path.join(path, filename_1)\n",
    "torch.save(best_acc_model_state, full_path_1)\n",
    "\n",
    "# Save model with highest valid MCC\n",
    "filename_2 = \"cointnet_mcc_cls_model_01.pth\"\n",
    "full_path_2 = os.path.join(path, filename_2)\n",
    "torch.save(best_mcc_model_state, full_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['loss', 'accuracy', 'mcc']\n",
    "fig, ax = plt.subplots(nrows=len(metric_names), ncols=1, figsize=(8, 6))\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    ax[i].set_title(m)\n",
    "    train_values = [t[i] for t in train_metrics]\n",
    "    valid_values = [t[i] for t in valid_metrics]\n",
    "    ax[i].plot(train_values, label='train')\n",
    "    ax[i].plot(valid_values, label='valid')\n",
    "    ax[i].set_xlabel('Epochs')\n",
    "    ax[i].set_ylabel(m)\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.subplots_adjust(wspace=0., hspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = full_path_1  #best valid acc model\n",
    "#MODEL_PATH = full_path_2  # best valid mcc model\n",
    "classifier = ContiNetClassification(num_points=NUM_TEST_POINTS, num_global_feats=GLOBAL_FEATS, k=NUM_CLASSES).to(DEVICE)\n",
    "classifier.load_state_dict(torch.load(MODEL_PATH))\n",
    "classifier.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test loop and get the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_batch = int(np.ceil(len(test_dataset)/BATCH_SIZE))\n",
    "\n",
    "with torch.no_grad():\n",
    "    epoch_loss, \\\n",
    "    epoch_accuracy, \\\n",
    "    epoch_mcc, \\\n",
    "    total_test_targets, \\\n",
    "    total_test_preds = train_test(classifier, test_dataloader,\n",
    "                                  num_test_batch, epoch=1,\n",
    "                                  split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test Loss: {epoch_loss:.4f} '\\\n",
    "      f'- Test Accuracy: {epoch_accuracy:.4f} '\\\n",
    "        f'- Test MCC: {epoch_mcc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(total_test_targets, total_test_preds, target_names=list(CATEGORIES.keys()))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "test_confusion = pd.DataFrame(confusion_matrix(total_test_targets, total_test_preds),\n",
    "                              columns=list(CATEGORIES.keys()),\n",
    "                              index=list(CATEGORIES.keys()))\n",
    "test_confusion.to_csv('confusion_matrix_for_mcc_model_00.csv')\n",
    "test_confusion\n",
    "# Columns represents Predictions and Rows represents labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat Map Analysis\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(test_confusion, annot=True, cmap='YlOrRd')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "per_class_acc = {}\n",
    "total_example = 0.\n",
    "correct_classified = 0.\n",
    "for category in (CATEGORIES.keys()):\n",
    "    true_positives = test_confusion.loc[category, category]\n",
    "    total_instances = test_confusion.loc[category].sum()\n",
    "    per_class_acc[category] = true_positives / total_instances\n",
    "    total_example += total_instances\n",
    "    correct_classified += true_positives\n",
    "\n",
    "overall_accuracy = correct_classified/total_example\n",
    "# Print per-class accuracy\n",
    "summation_accuray = 0\n",
    "print(\"Per-class Accuracy:\")\n",
    "\n",
    "for category, accuracy in per_class_acc.items():\n",
    "    accuracy = float(accuracy)\n",
    "    summation_accuray += accuracy\n",
    "    print(f\"{category}: {accuracy:.4f}\")\n",
    "\n",
    "average_accuracy = summation_accuray/len(CATEGORIES)\n",
    "print(\"\\nAverage Accuracy accros all classes :\", average_accuracy.__round__(4))\n",
    "print(\"Overall Accuracy: \", overall_accuracy.__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "torch.cuda.empty_cache()    # release GPU memory\n",
    "\n",
    "# test Dataset (Segmentation version for display)\n",
    "test_sample_dataset = Modelnet40Dataset(ROOT, npoints=NUM_TEST_POINTS, split='test',\n",
    "                                       normalize=True)\n",
    "\n",
    "# get random sample from test data\n",
    "random_idx = randrange(len(test_sample_dataset))\n",
    "points, seg = test_sample_dataset.__getitem__(random_idx)\n",
    "\n",
    "# normalize points\n",
    "#norm_points = test_sample_dataset.normalize_points(points)\n",
    "norm_points = points\n",
    "\n",
    "with torch.no_grad():\n",
    "    norm_points = norm_points.unsqueeze(0).transpose(2, 1).to(DEVICE)\n",
    "    targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "    preds, crit_idxs, _ = classifier(norm_points)\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    pred_choice = preds.squeeze().argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = list(CATEGORIES.keys())[pred_choice.cpu().numpy()]\n",
    "pred_prob = preds[0, pred_choice]\n",
    "print(f'The predicted class is: {pred_class}, with probability: {pred_prob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(CATEGORIES.values()), preds.cpu().numpy()[0]);\n",
    "plt.xticks(list(CATEGORIES.values()), list(CATEGORIES.keys()), rotation=90)\n",
    "plt.title('Predicted Classes')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Probabilities');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(norm_points[0, :, :].cpu().numpy().T)\n",
    "pcd.points = o3.utility.Vector3dVector(points.cpu().numpy())\n",
    "#pcd.colors = o3.utility.Vector3dVector(read_pointnet_colors(seg.numpy()))\n",
    "for i, j in CATEGORIES.items():\n",
    "    if seg == j:\n",
    "        label = i\n",
    "print(\"Original class: \",label.title() )\n",
    "o3.visualization.draw_plotly([pcd])\n",
    "#draw(pcd, point_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the critical sets\n",
    "\n",
    "The critical sets are the points that make up the basic underlying structure of the point cloud. Now we will see how well the model has learned these.\n",
    "\n",
    "See draw_plotly() source here: https://github.com/isl-org/Open3D/blob/master/python/open3d/visualization/draw_plotly.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_points = points[crit_idxs.squeeze().to(points.device), :]\n",
    "#critical_point_colors = read_pointnet_colors(seg.numpy())[crit_idxs.cpu().squeeze(), :]\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "#pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "o3.visualization.draw_plotly([pcd])\n",
    "#draw(pcd, point_size=5) # does not work in Colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
